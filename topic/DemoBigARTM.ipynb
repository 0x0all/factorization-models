{
 "metadata": {
  "name": "",
  "signature": "sha256:a240c08b05961a6a047e7c6427ac261b38af7d25fb982dc4606559d7fbc13dfe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "BigARTM configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "\n",
      "BIGARTM_PATH = '/home/romovpa/bigartm'\n",
      "BIGARTM_BUILD_PATH = '/home/romovpa/bigartm/build'\n",
      "\n",
      "sys.path.append(os.path.join(BIGARTM_PATH, 'src/python'))\n",
      "os.environ['ARTM_SHARED_LIBRARY'] = os.path.join(BIGARTM_BUILD_PATH, 'src/artm/libartm.so')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 01: Synthetic Collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import artm.messages_pb2, artm.library, sys, time, random, glob, math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate small collection of random items\n",
      "num_tokens = 60\n",
      "num_items = 100\n",
      "batch = artm.messages_pb2.Batch()\n",
      "for token_id in range(0, num_tokens):\n",
      "  batch.token.append('token' + str(token_id))\n",
      "\n",
      "for item_id in range(0, num_items):\n",
      "  item = batch.item.add()\n",
      "  item.id = item_id\n",
      "  field = item.field.add()\n",
      "  for token_id in range(0, num_tokens):\n",
      "    field.token_id.append(token_id)\n",
      "    background_count = random.randint(1, 5) if (token_id >= 40) else 0\n",
      "    topical_count    = 10 if ((token_id < 40) and ((token_id % 10) == (item_id % 10))) else 0\n",
      "    field.token_count.append(background_count + topical_count)\n",
      "\n",
      "# Create master component and infer topic model\n",
      "with artm.library.MasterComponent() as master:\n",
      "  master.AddBatch(batch)\n",
      "  perplexity_score = master.CreatePerplexityScore()\n",
      "  top_tokens_score = master.CreateTopTokensScore(num_tokens = 4)\n",
      "  model = master.CreateModel(topics_count = 10, inner_iterations_count = 10)\n",
      "  model.EnableScore(perplexity_score)\n",
      "  model.EnableScore(top_tokens_score)\n",
      "\n",
      "  for iter in range(0, 10):\n",
      "    master.InvokeIteration(1)        # Invoke one scan of the entire collection...\n",
      "    master.WaitIdle();               # and wait until it completes.\n",
      "    model.Synchronize();             # Synchronize topic model.\n",
      "    print \"Iter#\" + str(iter) + \": Perplexity = %.3f\" % perplexity_score.GetValue(model).value\n",
      "\n",
      "  top_tokens = top_tokens_score.GetValue(model)\n",
      "\n",
      "artm.library.Visualizers.PrintTopTokensScore(top_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "libartm.so: cannot open shared object file: No such file or directory, fall back to ARTM_SHARED_LIBRARY environment variable\n",
        "Iter#0: Perplexity = 0.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter#1: Perplexity = 56.386\n",
        "Iter#2: Perplexity = 37.929\n",
        "Iter#3: Perplexity = 28.446\n",
        "Iter#4: Perplexity = 24.141\n",
        "Iter#5: Perplexity = 22.017\n",
        "Iter#6: Perplexity = 21.744\n",
        "Iter#7: Perplexity = 21.708"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iter#8: Perplexity = 21.676\n",
        "Iter#9: Perplexity = 21.598\n",
        "\n",
        "Top tokens per topic: \n",
        "Topic#1:  token24(0.10)  token46(0.08)  token44(0.07)  token43(0.07)  \n",
        "Topic#5:  token30(0.10)  token20(0.10)  token10(0.10)  token0(0.10)  \n",
        "Topic#9:  token8(0.10)  token38(0.10)  token28(0.10)  token18(0.10)  \n",
        "Topic#13:  token32(0.10)  token22(0.10)  token2(0.10)  token12(0.10)  \n",
        "Topic#17:  token7(0.10)  token37(0.10)  token27(0.10)  token17(0.10)  \n",
        "Topic#21:  token35(0.10)  token5(0.10)  token25(0.10)  token15(0.10)  \n",
        "Topic#25:  token23(0.10)  token3(0.10)  token33(0.10)  token13(0.10)  \n",
        "Topic#29:  token6(0.10)  token36(0.10)  token26(0.10)  token16(0.10)  \n",
        "Topic#33:  token9(0.07)  token39(0.07)  token29(0.07)  token19(0.07)  \n",
        "Topic#37:  token31(0.10)  token21(0.10)  token11(0.10)  token1(0.10) \n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}